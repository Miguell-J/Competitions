{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3917776,"sourceType":"datasetVersion","datasetId":2326580},{"sourceId":85986,"sourceType":"modelInstanceVersion","modelInstanceId":72246,"modelId":78150}],"dockerImageVersionId":30827,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine-tuning Gemma for Chinese Poetry Generation\n\nThis document describes how to fine-tune the **Gemma** model using a dataset of Chinese poetry. The goal is to adapt the model to generate Chinese poetry in a classical style by training it on a subset of poems. The fine-tuning process leverages **LoRA** (Low-Rank Adaptation) for efficient model adaptation.","metadata":{}},{"cell_type":"markdown","source":"---\n\n## Prerequisites\n\nMake sure to install the required libraries using the following commands:","metadata":{}},{"cell_type":"code","source":"!pip install -q -U keras-nlp datasets\n!pip install -q -U keras","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-13T18:50:20.326259Z","iopub.execute_input":"2025-01-13T18:50:20.326570Z","iopub.status.idle":"2025-01-13T18:50:31.117093Z","shell.execute_reply.started":"2025-01-13T18:50:20.326546Z","shell.execute_reply":"2025-01-13T18:50:31.115942Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"---\n\n\n## Setup and Configuration\n\nBefore fine-tuning the model, we configure the backend and environment variables. This step is essential for optimizing memory usage and performance:\n\n```python\nimport keras_nlp\nimport keras\nimport os\n\nos.environ[\"KERAS_BACKEND\"] = \"jax\"  # Use JAX as backend for optimization\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\"  # Max memory usage","metadata":{}},{"cell_type":"code","source":"import keras_nlp\nimport keras\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T18:50:31.118530Z","iopub.execute_input":"2025-01-13T18:50:31.118873Z","iopub.status.idle":"2025-01-13T18:50:40.577743Z","shell.execute_reply.started":"2025-01-13T18:50:31.118849Z","shell.execute_reply":"2025-01-13T18:50:40.577037Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"os.environ[\"KERAS_BACKEND\"] = \"jax\"\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T18:50:40.579216Z","iopub.execute_input":"2025-01-13T18:50:40.579743Z","iopub.status.idle":"2025-01-13T18:50:40.583311Z","shell.execute_reply.started":"2025-01-13T18:50:40.579698Z","shell.execute_reply":"2025-01-13T18:50:40.582475Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"---\n\n## Hyperparameters and Model Configuration\n\nHere, we define important parameters such as token limits, dataset size, LoRA rank, learning rate, and the number of training epochs:\n\n```python\ntoken_limit = 128  # Max token length per input\nnum_data_limit = 500  # Number of training examples to use\nlora_rank = 4  # LoRA rank for model adaptation\nlr_value = 1e-4  # Learning rate for training\ntrain_epoch = 2  # Number of epochs for fine-tuning\nmodel_id = \"gemma2_instruct_2b_en\"  # Pre-trained model ID for fine-tuning\n```\n\nThese parameters allow you to control the model's training behavior and performance.","metadata":{}},{"cell_type":"code","source":"token_limit = 128 \nnum_data_limit = 500 \nlora_rank = 4 \nlr_value = 1e-4 \ntrain_epoch = 2 \nmodel_id = \"gemma2_instruct_2b_en\" ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T18:50:40.584942Z","iopub.execute_input":"2025-01-13T18:50:40.585347Z","iopub.status.idle":"2025-01-13T18:50:40.609005Z","shell.execute_reply.started":"2025-01-13T18:50:40.585306Z","shell.execute_reply":"2025-01-13T18:50:40.607987Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"---\n\n## Loading the Pre-trained Gemma Model\n\nNext, we load the pre-trained **Gemma** model and check its architecture. The model is saved into a directory called `gemma2_chinese_poetry`:","metadata":{}},{"cell_type":"code","source":"model_folder = \"gemma2_chinese_poetry\"\nif not os.path.exists(model_folder):\n    os.mkdir(model_folder)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T18:50:40.610148Z","iopub.execute_input":"2025-01-13T18:50:40.610403Z","iopub.status.idle":"2025-01-13T18:50:40.623987Z","shell.execute_reply.started":"2025-01-13T18:50:40.610381Z","shell.execute_reply":"2025-01-13T18:50:40.623113Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma2_instruct_2b_en\")\ngemma_lm.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T18:50:40.625128Z","iopub.execute_input":"2025-01-13T18:50:40.625476Z","iopub.status.idle":"2025-01-13T18:51:36.123070Z","shell.execute_reply.started":"2025-01-13T18:50:40.625446Z","shell.execute_reply":"2025-01-13T18:51:36.122190Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                              │                      Vocab size: \u001b[38;5;34m256,000\u001b[0m │\n└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                              │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)        │   \u001b[38;5;34m2,614,341,888\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m589,824,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"---\n\n## Data Preprocessing\n\nWe load and preprocess the dataset, which consists of Chinese poems. We read the poems, clean them, and prepare them for training. Only poems that meet the token length requirement are kept. The poems are read from the file, and only the ones with a length less than the specified token limit are used for training.","metadata":{}},{"cell_type":"code","source":"with open('/kaggle/input/chinesepoetrydataset/chinese_poems.txt', 'r', encoding='utf-8') as f:\n    poems = f.readlines()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T18:51:36.124044Z","iopub.execute_input":"2025-01-13T18:51:36.124379Z","iopub.status.idle":"2025-01-13T18:51:36.983180Z","shell.execute_reply.started":"2025-01-13T18:51:36.124346Z","shell.execute_reply":"2025-01-13T18:51:36.982502Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"poems = poems[:num_data_limit]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T18:51:36.983969Z","iopub.execute_input":"2025-01-13T18:51:36.984256Z","iopub.status.idle":"2025-01-13T18:51:37.003901Z","shell.execute_reply.started":"2025-01-13T18:51:36.984228Z","shell.execute_reply":"2025-01-13T18:51:37.003006Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"train = []\nfor poem in poems[:num_data_limit]:\n    poem = poem.strip()  # Remover espaços extras\n    if len(poem.split()) < token_limit:  # Garantir que o comprimento não exceda o limite\n        train.append(f\"<start_of_turn>user\\n{poem}\\n<end_of_turn>\\n<start_of_turn>model\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T18:51:37.006622Z","iopub.execute_input":"2025-01-13T18:51:37.006876Z","iopub.status.idle":"2025-01-13T18:51:37.022461Z","shell.execute_reply.started":"2025-01-13T18:51:37.006855Z","shell.execute_reply":"2025-01-13T18:51:37.021791Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"print(f\"Number of training examples: {len(train)}\")\nprint(f\"First example: {train[0]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T18:51:37.023741Z","iopub.execute_input":"2025-01-13T18:51:37.023941Z","iopub.status.idle":"2025-01-13T18:51:37.038780Z","shell.execute_reply.started":"2025-01-13T18:51:37.023924Z","shell.execute_reply":"2025-01-13T18:51:37.037896Z"}},"outputs":[{"name":"stdout","text":"Number of training examples: 500\nFirst example: <start_of_turn>user\n欲出未出光辣达,千山万山如火发.须臾走向天上来,逐却残星赶却月.\n<end_of_turn>\n<start_of_turn>model\n\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"---\n\n## Model Fine-tuning with LoRA\n\nThe **LoRA** technique is applied to the model to adapt it efficiently with fewer parameters. We also configure the optimizer, which uses **AdamW** for weight decay regularization. The model is fine-tuned using the defined parameters and optimizer. After training, the LoRA weights are saved to a file for later use.","metadata":{}},{"cell_type":"code","source":"gemma_lm.backbone.enable_lora(rank=lora_rank)\ngemma_lm.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T18:51:37.039526Z","iopub.execute_input":"2025-01-13T18:51:37.039819Z","iopub.status.idle":"2025-01-13T18:51:37.254015Z","shell.execute_reply.started":"2025-01-13T18:51:37.039800Z","shell.execute_reply":"2025-01-13T18:51:37.253331Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                              │                      Vocab size: \u001b[38;5;34m256,000\u001b[0m │\n└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                              │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)        │   \u001b[38;5;34m2,617,270,528\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m589,824,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,617,270,528</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,617,270,528\u001b[0m (9.75 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,617,270,528</span> (9.75 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,928,640\u001b[0m (11.17 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,928,640</span> (11.17 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n</pre>\n"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"gemma_lm.preprocessor.sequence_length = token_limit","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T18:51:37.254670Z","iopub.execute_input":"2025-01-13T18:51:37.254895Z","iopub.status.idle":"2025-01-13T18:51:37.258570Z","shell.execute_reply.started":"2025-01-13T18:51:37.254876Z","shell.execute_reply":"2025-01-13T18:51:37.257629Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"optimizer = keras.optimizers.AdamW(\n    learning_rate=lr_value,\n    weight_decay=0.01,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T18:51:37.259412Z","iopub.execute_input":"2025-01-13T18:51:37.259621Z","iopub.status.idle":"2025-01-13T18:51:37.278031Z","shell.execute_reply.started":"2025-01-13T18:51:37.259603Z","shell.execute_reply":"2025-01-13T18:51:37.277227Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T18:51:37.278828Z","iopub.execute_input":"2025-01-13T18:51:37.279054Z","iopub.status.idle":"2025-01-13T18:51:37.283950Z","shell.execute_reply.started":"2025-01-13T18:51:37.279024Z","shell.execute_reply":"2025-01-13T18:51:37.283182Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"gemma_lm.fit(train, epochs=train_epoch, batch_size=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T18:51:37.284586Z","iopub.execute_input":"2025-01-13T18:51:37.284853Z","iopub.status.idle":"2025-01-13T18:57:00.039997Z","shell.execute_reply.started":"2025-01-13T18:51:37.284834Z","shell.execute_reply":"2025-01-13T18:57:00.039135Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/2\n\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 245ms/step - loss: 3.6435 - sparse_categorical_accuracy: 0.1217\nEpoch 2/2\n\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 245ms/step - loss: 2.5870 - sparse_categorical_accuracy: 0.2798\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7ae00f548d60>"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"---\n\n## Saving the Fine-tuned Model\n\nOnce the model is fine-tuned, we save the **LoRA weights** to a file so that the trained model can be reused:","metadata":{}},{"cell_type":"code","source":"gemma_lm.backbone.save_lora_weights(f\"/kaggle/working/gemma2_chinese_poetry/gemma2_chinese_poetry.lora.h5\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T18:57:00.040933Z","iopub.execute_input":"2025-01-13T18:57:00.041296Z","iopub.status.idle":"2025-01-13T18:57:00.132087Z","shell.execute_reply.started":"2025-01-13T18:57:00.041259Z","shell.execute_reply":"2025-01-13T18:57:00.131175Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"---\n\n## Text Generation with the Fine-tuned Model\n\nFinally, the model can generate text based on a given prompt. We define a function to generate text using the fine-tuned model, which takes an input prompt and returns a generated response:","metadata":{}},{"cell_type":"code","source":"def generate_text(prompt, token_limit=256):\n    input_text = f\"<start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n\"\n    generated_text = gemma_lm.generate(input_text, max_length=token_limit)\n    print(\"\\nGenerated text:\")\n    print(generated_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T18:57:00.133140Z","iopub.execute_input":"2025-01-13T18:57:00.133402Z","iopub.status.idle":"2025-01-13T18:57:00.137231Z","shell.execute_reply.started":"2025-01-13T18:57:00.133381Z","shell.execute_reply":"2025-01-13T18:57:00.136556Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"---\n\n## Example Usage\n\nHere’s an example of how to generate a Chinese poem using the fine-tuned model. The prompt is in Chinese and asks the model to write a Tang Dynasty-style poem about the moon:","metadata":{}},{"cell_type":"code","source":"prompt = \"写一首关于月亮的唐代风格诗。\"\ngenerate_text(prompt)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T18:57:00.138014Z","iopub.execute_input":"2025-01-13T18:57:00.138258Z","iopub.status.idle":"2025-01-13T18:57:35.295997Z","shell.execute_reply.started":"2025-01-13T18:57:00.138237Z","shell.execute_reply":"2025-01-13T18:57:35.295111Z"}},"outputs":[{"name":"stdout","text":"\nGenerated text:\n<start_of_turn>user\n写一首关于月亮的唐代风格诗。<end_of_turn>\n<start_of_turn>model\n## 月夜吟\n\n银盘高悬夜空深,\n清辉洒下寒枝眠.\n孤灯摇曳花枝摇,\n月影映照水波流.\n静待春风拂柳,\n长醉夜色无相思.<end_of_turn>\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"---\n\n## Conclusion\n\nBy following this guide, you can fine-tune the **Gemma** model for various tasks, such as generating Chinese poetry. Using techniques like **LoRA** for efficient adaptation, the model can be specialized to produce creative outputs while maintaining performance.","metadata":{"execution":{"iopub.status.busy":"2025-01-13T18:57:35.297394Z","iopub.execute_input":"2025-01-13T18:57:35.297672Z","iopub.status.idle":"2025-01-13T18:57:35.303388Z","shell.execute_reply.started":"2025-01-13T18:57:35.297647Z","shell.execute_reply":"2025-01-13T18:57:35.302254Z"}}}]}